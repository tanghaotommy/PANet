{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from config import ex, cfg\n",
    "from torchvision.transforms import Compose\n",
    "from dataloaders.customized import voc_fewshot, coco_fewshot\n",
    "from dataloaders.transforms import RandomMirror, Resize, ToTensorNormalize\n",
    "from dataloaders.transforms import RandomMirror, Resize, ToTensorNormalize\n",
    "from util.utils import set_seed, CLASS_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_config = cfg()\n",
    "data_name = _config['dataset']\n",
    "transforms = Compose([Resize(size=_config['input_size']),\n",
    "                        RandomMirror()])\n",
    "labels = CLASS_LABELS[data_name][_config['label_sets']]\n",
    "\n",
    "dataset = voc_fewshot(\n",
    "    base_dir=_config['path'][data_name]['data_dir'],\n",
    "    split=_config['path'][data_name]['data_split'],\n",
    "    transforms=transforms,\n",
    "    to_tensor=ToTensorNormalize(),\n",
    "    labels=labels,\n",
    "    max_iters=_config['n_steps'] * _config['batch_size'],\n",
    "    n_ways=_config['task']['n_ways'],\n",
    "    n_shots=_config['task']['n_shots'],\n",
    "    n_queries=_config['task']['n_queries']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "support_image = data['support_images']\n",
    "support_mask = data['support_mask']\n",
    "n_way = len(support_image)\n",
    "n_shot = len(support_image[0])\n",
    "print(n_way, n_shot)\n",
    "\n",
    "img = np.moveaxis(support_image[0][0].numpy(), 0, -1)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "mask = support_mask[0][0]['bg_mask'].numpy()\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\"\"\"Evaluation Script\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from config import ex, cfg\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from models.fewshot import FewShotSeg\n",
    "from dataloaders.customized import voc_fewshot, coco_fewshot\n",
    "from dataloaders.transforms import ToTensorNormalize\n",
    "from dataloaders.transforms import Resize, DilateScribble\n",
    "from util.metric import Metric\n",
    "from util.utils import set_seed, CLASS_LABELS, get_bbox\n",
    "from config import ex\n",
    "\n",
    "_config = cfg()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "model = FewShotSeg(pretrained_path=_config['path']['init_path'], cfg=_config['model'])\n",
    "model = nn.DataParallel(model.cuda())\n",
    "if not _config['notrain']:\n",
    "    snapshot = './runs/PANet_VOC_align_sets_0_1way_1shot_[train]/3/snapshots/30000.pth'\n",
    "    print('Load PANet model from ', snapshot)\n",
    "    model.load_state_dict(torch.load(snapshot, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "alp_model_config = _config['model']\n",
    "alp_model_config['use_alp'] = True\n",
    "model_alp = FewShotSeg(pretrained_path=_config['path']['init_path'], cfg=alp_model_config)\n",
    "model_alp = nn.DataParallel(model_alp.cuda())\n",
    "if not _config['notrain']:\n",
    "    snapshot = './runs/PANet_VOC_align_use_alp_sets_0_1way_1shot_masked_no_back_alp_4_[train]/1/snapshots/30000.pth'\n",
    "    print('Load alp model from ', snapshot)\n",
    "    model_alp.load_state_dict(torch.load(snapshot, map_location='cpu'))\n",
    "model_alp.eval()\n",
    "\n",
    "data_name = _config['dataset']\n",
    "if data_name == 'VOC':\n",
    "    make_data = voc_fewshot\n",
    "    max_label = 20\n",
    "elif data_name == 'COCO':\n",
    "    make_data = coco_fewshot\n",
    "    max_label = 80\n",
    "else:\n",
    "    raise ValueError('Wrong config for dataset!')\n",
    "labels = CLASS_LABELS[data_name]['all'] - CLASS_LABELS[data_name][_config['label_sets']]\n",
    "transforms = [Resize(size=_config['input_size'])]\n",
    "if _config['scribble_dilation'] > 0:\n",
    "    transforms.append(DilateScribble(size=_config['scribble_dilation']))\n",
    "transforms = Compose(transforms)\n",
    "\n",
    "\n",
    "metric = Metric(max_label=max_label, n_runs=_config['n_runs'])\n",
    "metric_alp = Metric(max_label=max_label, n_runs=_config['n_runs'])\n",
    "with torch.no_grad():\n",
    "    run = 0\n",
    "    # set_seed(_config['seed'] + run)\n",
    "\n",
    "    print(f'### Load data ###')\n",
    "    dataset = make_data(\n",
    "        base_dir=_config['path'][data_name]['data_dir'],\n",
    "        split=_config['path'][data_name]['data_split'],\n",
    "        transforms=transforms,\n",
    "        to_tensor=ToTensorNormalize(),\n",
    "        labels=labels,\n",
    "        max_iters=_config['n_steps'] * _config['batch_size'],\n",
    "        n_ways=_config['task']['n_ways'],\n",
    "        n_shots=_config['task']['n_shots'],\n",
    "        n_queries=_config['task']['n_queries']\n",
    "    )\n",
    "    if _config['dataset'] == 'COCO':\n",
    "        coco_cls_ids = dataset.datasets[0].dataset.coco.getCatIds()\n",
    "    testloader = DataLoader(dataset, batch_size=_config['batch_size'], shuffle=True,\n",
    "                            num_workers=1, pin_memory=True, drop_last=False)\n",
    "    print(f\"Total # of Data: {len(dataset)}\")\n",
    "\n",
    "    iter_object = testloader.__iter__()\n",
    "    sample_batched = next(iter_object)\n",
    "    sample_batched = next(iter_object)\n",
    "\n",
    "    if _config['dataset'] == 'COCO':\n",
    "        label_ids = [coco_cls_ids.index(x) + 1 for x in sample_batched['class_ids']]\n",
    "    else:\n",
    "        label_ids = list(sample_batched['class_ids'])\n",
    "    support_images = [[shot.cuda() for shot in way]\n",
    "                        for way in sample_batched['support_images']]\n",
    "    suffix = 'scribble' if _config['scribble'] else 'mask'\n",
    "\n",
    "    if _config['bbox']:\n",
    "        support_fg_mask = []\n",
    "        support_bg_mask = []\n",
    "        for i, way in enumerate(sample_batched['support_mask']):\n",
    "            fg_masks = []\n",
    "            bg_masks = []\n",
    "            for j, shot in enumerate(way):\n",
    "                fg_mask, bg_mask = get_bbox(shot['fg_mask'],\n",
    "                                            sample_batched['support_inst'][i][j])\n",
    "                fg_masks.append(fg_mask.float().cuda())\n",
    "                bg_masks.append(bg_mask.float().cuda())\n",
    "            support_fg_mask.append(fg_masks)\n",
    "            support_bg_mask.append(bg_masks)\n",
    "    else:\n",
    "        support_fg_mask = [[shot[f'fg_{suffix}'].float().cuda() for shot in way]\n",
    "                            for way in sample_batched['support_mask']]\n",
    "        support_bg_mask = [[shot[f'bg_{suffix}'].float().cuda() for shot in way]\n",
    "                            for way in sample_batched['support_mask']]\n",
    "\n",
    "    query_images = [query_image.cuda()\n",
    "                    for query_image in sample_batched['query_images']]\n",
    "    query_labels = torch.cat(\n",
    "        [query_label.cuda()for query_label in sample_batched['query_labels']], dim=0)\n",
    "\n",
    "    query_pred_alp, _ = model_alp(support_images, support_fg_mask, support_bg_mask,\n",
    "                            query_images)\n",
    "                            \n",
    "    query_pred, _ = model(support_images, support_fg_mask, support_bg_mask,\n",
    "                            query_images)\n",
    "\n",
    "    metric.record(np.array(query_pred.argmax(dim=1)[0].cpu()),\n",
    "                    np.array(query_labels[0].cpu()),\n",
    "                    labels=label_ids, n_run=run)\n",
    "\n",
    "    classIoU, meanIoU = metric.get_mIoU(labels=sorted(labels), n_run=run)\n",
    "    classIoU_binary, meanIoU_binary = metric.get_mIoU_binary(n_run=run)\n",
    "    print('Original')\n",
    "    print(f'classIoU: {classIoU}')\n",
    "    print(f'meanIoU: {meanIoU}')\n",
    "    print(f'classIoU_binary: {classIoU_binary}')\n",
    "    print(f'meanIoU_binary: {meanIoU_binary}')\n",
    "\n",
    "    print('Adaptive Local pooling')\n",
    "    metric_alp.record(np.array(query_pred_alp.argmax(dim=1)[0].cpu()),\n",
    "                    np.array(query_labels[0].cpu()),\n",
    "                    labels=label_ids, n_run=run)\n",
    "\n",
    "    classIoU, meanIoU = metric_alp.get_mIoU(labels=sorted(labels), n_run=run)\n",
    "    classIoU_binary, meanIoU_binary = metric_alp.get_mIoU_binary(n_run=run)\n",
    "    print(f'classIoU: {classIoU}')\n",
    "    print(f'meanIoU: {meanIoU}')\n",
    "    print(f'classIoU_binary: {classIoU_binary}')\n",
    "    print(f'meanIoU_binary: {meanIoU_binary}')"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(support_fg_mask[0][0][0].cpu().numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "support_image = support_images[0][0][0].permute(1, 2, 0).cpu().numpy()\n",
    "support_image = (support_image - support_image.min()) / (support_image.max() - support_image.min())\n",
    "plt.imshow(support_image)\n",
    "plt.title('Support image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(query_labels[0].cpu().numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "query_image = query_images[0][0].permute(1, 2, 0).cpu().numpy()\n",
    "query_image = (query_image - query_image.min()) / (query_image.max() - query_image.min())\n",
    "plt.imshow(query_image)\n",
    "plt.title('Query image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(query_pred.softmax(dim=1)[0][1].cpu().numpy())\n",
    "plt.title(f'PANet (IoU = {round(np.nanmean(metric.get_mIoU(labels=sorted(labels), n_run=run)[0]), 3)})')\n",
    "plt.colorbar(shrink=0.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(query_pred_alp.softmax(dim=1)[0][1].cpu().numpy())\n",
    "plt.title(f'PANet + ALP (IoU = {round(np.nanmean(metric_alp.get_mIoU(labels=sorted(labels), n_run=run)[0]), 3)})')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.10 64-bit ('panet': conda)",
   "display_name": "Python 3.6.10 64-bit ('panet': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97e347d35cec56b114ee4e2c1666a2f0b6d4127f413b187a11870608af7f70d2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}